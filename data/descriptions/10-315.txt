Course Code: 10-315
Course Name: 10-315, Spring 2025
Course URL: https://www.cs.cmu.edu/~10315/
================================================================================

OVERVIEW:
--------------------------------------------------------------------------------
Key InformationLectureMonday + Wednesday, 11:00 am - 12:20 pm, Wean Hall 7500RecitationSection A, Friday 10:00 am - 10:50 am, GHC 4102, seeRecitationSection B/C, Friday 11:00 am - 11:50 am, GHC 4301Section D, Friday 12:00 pm - 12:50 pm, GHC 4301Section E, Friday 1:00 pm - 1:50 pm, PH 126ASection F, Friday 2:00 pm - 2:50 pm, GHC 4102InstructorPat VirtueEducation AssociateNichelle PhillipsTeaching AssistantsMargaret He,
                    Derek Yuan,
                    Shreya Sridhar,
                    Gaurika Sawhney,
                    Jerick Shi,
                    Ethan Wang,
                    AJ Seo, see the315 Staff pageGradingGrades will be collected inCanvas.Midterm 1 20%, Midterm 2 20%, Written/Programming Homework 35%, Pre-Lecture Reading Checkpoints 5%, Online Homework 5%, Participation 5%, Mini-project 10%TextbookThere is no required textbook for this course. Any recommended readings will come from sources freely available online.Announcements + Q&AWe will useEdfor questions and any course announcements.Submitting AssignmentsStudents will turn in their homework electronically usingGradescope.
Machine Learning is concerned with computer programs that automatically improve their performance through experience (e.g., programs that learn to recognize human faces, recommend music and movies, and drive autonomous robots).
          This course covers the core concepts, theory, algorithms and applications of machine learning.

DESCRIPTION:
--------------------------------------------------------------------------------
10-315, Spring 2025Introduction to Machine Learning (SCS majors)OverviewKey InformationLectureMonday + Wednesday, 11:00 am - 12:20 pm, Wean Hall 7500RecitationSection A, Friday 10:00 am - 10:50 am, GHC 4102, seeRecitationSection B/C, Friday 11:00 am - 11:50 am, GHC 4301Section D, Friday 12:00 pm - 12:50 pm, GHC 4301Section E, Friday 1:00 pm - 1:50 pm, PH 126ASection F, Friday 2:00 pm - 2:50 pm, GHC 4102InstructorPat VirtueEducation AssociateNichelle PhillipsTeaching AssistantsMargaret He,
                    Derek Yuan,
                    Shreya Sridhar,
                    Gaurika Sawhney,
                    Jerick Shi,
                    Ethan Wang,
                    AJ Seo, see the315 Staff pageGradingGrades will be collected inCanvas.Midterm 1 20%, Midterm 2 20%, Written/Programming Homework 35%, Pre-Lecture Reading Checkpoints 5%, Online Homework 5%, Participation 5%, Mini-project 10%TextbookThere is no required textbook for this course. Any recommended readings will come from sources freely available online.Announcements + Q&AWe will useEdfor questions and any course announcements.Submitting AssignmentsStudents will turn in their homework electronically usingGradescope.Machine Learning is concerned with computer programs that automatically improve their performance through experience (e.g., programs that learn to recognize human faces, recommend music and movies, and drive autonomous robots).
          This course covers the core concepts, theory, algorithms and applications of machine learning.Learning ObjectivesAfter completing the course, students should be able to:Select and apply an appropriate supervised learning algorithm for classification and regression problems (e.g., linear regression, logistic regression, ridge regression, nonparametric kernel regression, neural networks, naive Bayes).Recognize different types of unsupervised learning problems, and select and apply appropriate algorithms (e.g., k-means clustering, Gaussian mixture models, linear and nonlinear dimensionality reduction).Work with probability (Bayes rule, conditioning, expectations, independence), linear algebra (vector and matrix operations, eigenvectors, SVD), and calculus (gradients, Jacobians) to derive machine learning methods such as linear regression, naive Bayes, and principal component analysis.Understand machine learning principles such as model selection, overfitting, and underfitting, and techniques such as cross-validation,regularization, feature learning, fine-tuning, and transfer learning.Implement machine learning algorithms such as logistic regression via stochastic gradient descent, linear regression, or k-means clustering.Work with machine learning toolkits, such as PyTorch, to implement, train, and analyze various deep learning networks, including convolutional neural networks and transformer architectures.Run appropriate supervised and unsupervised learning algorithms on real and synthetic data sets and interpret the results.LevelsThis course is designed for SCS undergraduate majors. It covers many similar topics to other introductory machine learning course, such as 10-301/10-601 and 10-701. This 10-315 course and 15-281 AI Representation and Problem Solving are designed to complement each other and provide both breadth and depth across AI and ML topics. Contact the instructor if you are concerned about which machine learning course is appropriate for you.PrerequisitesThe prequisites for this course are:15-122:Principles of Imperative Computation15-151 or 21-127 or 21-128:Mathematical Foundations of Computer Science / Concepts of Mathematics.36-225 or 36-218 or 36-217 or 15-259 or 15-359 or 21-325 or 36-219: Probability21-241 or 21-240 or 21-242: Linear Algebra[Implied from prereqs above] Calc II Integration and ApproximationWhile not explicitly a prerequisite, we will be programming exclusively in Python. Please see the instructor if you are unsure whether your background is suitable for the course.Office HoursOHQueue:LinkPat's Office HoursIn addition to Pat's standing office hourse, he often have "OH" (or "Open") appointment slots on hisoffice hours appointment calendar.
        If no there are no available OH or appointments that meet your needs, please contact Pat via a private post onEdwith a list of times that work for you to meet.ScheduleSubject to changeTextbooks:Bishop, Christopher.Pattern Recognition and Machine Learning,available onlineDaumé III, Hal.A Course in Machine Learning,available online(DL) Goodfellow, Ian, Yoshua Bengio, Aaron Courville.Deep Learning,available online(MML) Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong.Mathematics for Machine Learning,available onlineMitchell, Tom.Machine Learning,available onlineMurphy, Kevin P.Machine Learning: A Probabilistic Perspective,available online(KMPA) Shaw-Taylor, John, Nello Cristianini.Kernel Methods for Pattern Analysis,available onlineDatesTopicLecture MaterialsPre-ReadingReading (optional)1/13 Mon1. IntroductionNotation_Guide.pdfMath_Background.pdfpptx(inked)pdf(inked)MML2.1-3, 2.5, 2.6 and 3.1, 3.2.1, 3.31/15 Wed2. ML Problem Formulationpptx(inked)pdf(inked)Mitchell1.1-1.2Daumé 11/20 MonNo class: MLK Day1/22 Wed3. Decision Treespptx(inked)pdf(inked)Decision Trees.pdfCheckpointdue 1/21 Tue, 11:59 pmDaumé 2,Entropy, Cross-Entropy video, A. GéronPaper:ID31/27 Mon4. K-Nearest Neighbor andModel Selectionpptx(inked)pdf(inked)pptx(inked)pdf(inked)Opt and LinReg.pdfCheckpointdue 1/26 Sun, 11:59 pmDaumé 3MML8.3.31/29 Wed5. Optimization and Linear Regressionpptx(inked)pdf(inked)regression interactive.ipynbregression blind interactive.ipynbMML8.2-8.2.2, 8.2.4MML5.2-5.52/3 Mon6. Optimization and Linear Regression (cont.)See previous lecture slidesFeatEng and LogReg.pdfCheckpointdue 2/2 Sun, 11:59 pm2/5 Wed7. Feature EngineeringLogistic Regressionpptx(inked)pdf(inked)Demos:Linear/Logistic:notebook,Desmos(3D)Quadratic/Logistic:notebook,Desmos 3DMulticlass logistic DesmosBishop4.1.3, 4.3.2, 4.3.42/10 Mon8. Logistic Regression (cont.)pptx(inked)pdf(inked)Convex functions DesmosNeural Networks.pdfCheckpointdue 2/9 Sun, 11:59 pm2/12 Wed9. Neural Networkspptx(inked)pdf(inked)three neuron interactive.ipynbMML5.6DL 6The Matrix Cookbook2/17 Mon10. Neural Networks (cont.)pptx(inked)pdf(inked)Universal network DesmosPerceptron neuron Desmos2/19 Wed11. Regularizationpptx(inked)pdf(inked)regression regularization.ipynbRegularization interpolation Desmos(3D)L1_sparsity.ipynbDL 7.1,7.8Bishop3.1.42/24 Mon12. MLE and Probabilistic Modelingpptx(inked)pdf(inked)MLE.pdfCheckpointdue 2/23 Sun, 11:59 pmMML9-9.2.2Bishop1.2.4-5, 3.1.1-22/26 WedEXAM 1In-classLearning objectives:pdfPractice exam:pdf(sol)3/3 MonNo class: Spring Break3/5 WedNo class: Spring Break3/10 Mon13. MLE (cont.) & MAPpptx(inked)pdf(inked)Desmos: Bernoulli Likelihood and PosteriorMAP.pdfCheckpointdue 3/9 Sun, 11:59 pmMML9.2.3-4Mitchell MLE and MAP3/12 Wed14. MAP (cont.)3/17 Mon15. Practical & Responsible MLPyTorch, ML Model Cardspptx(inked)pdf(inked)Demos:autoencoder csv.ipynbdigit logistic regression.ipynbdigit net.ipynbCNNs.pdfCheckpointdue3/17 Mon, 10:30 amModel Cards For Model Reporting. Margaret Mitchell, et al (2019)PyTorch Basics Tutorial3/19 Wed16. Deep Learning for Computer Visionpptx(inked)pdf(inked)DL 93/24 Mon17. Natrual Language Processing: Features, N-gramspptx(inked)pdf(inked)Demo:N-gramsWord Embeddings_v0_1.pdfNo checkpoint3/26 Wed18. NLP: Word Embeddings, Attentionpptx(inked)pdf(inked)Demos:Cosine SimilarityRotary Position Encoding 2DThe IllustratedWord2vec. Jay AlammarThe Illustrated {Attention→Transformer→GPT-2}. Jay AlammarVideo (and code):Let's build GPT. Andrej Karpathy3/31 Mon19. NLP (cont.): Transformers, LLMspptx(inked)pdf(inked)Demos:Linear TransformsLinear Transforms (words)PCA, Recommender Systems, ClusteringCheckpointdue 3/30 Sun, 11:59 pm4/2 Wed20. Dimensionality Reduction: PCA, Autoencoders, Feature Learningpptx(inked)pdf(inked)
              Demos:Desmos: Projectionpca_2d_exercise.ipynbautoencoder_csv.ipynbPCA:Bishop12.1,Murphy12.2Murphy25.54/7 Mon21. PCA (cont.) & Clusteringpptx(inked)pdf(inked)Generative.pdfCheckpointdue 4/6 Sun, 11:59 pmK-means:Bishop9.1,4/9 Wed22. Recommender Systems &Probabilistic Generative ModelsRecommender Systemspptx(inked)pdf(inked)Probabilistic Generative Modelspptx(inked)pdf(inked)Demos:cat_dog_sampling.ipynbdiscriminant_analysis.ipynbMatrix Factorization Techniques for Recommender Systems(pdf). Koren, Bell, and Volinsky (2009)Generative models:Mitchell Generative and Discriminative ClassifiersMurphy3.5, 4.2, 8.64/14 Mon23. Variational Autoencoderspptx(inked)pdf(inked)VAE.pdfCheckpointdue4/14 Mon, 10:30 amAuto-Encoding Variational BayesKingma and Welling, 2013.Tutorial on Variational AutoencodersDoersch, 2016.4/16 Wed24. Diffusion4/21 Mon25. Learning TheoryA Few Useful Things to Know about Machine Learning. Pedro Domingos (2012).Generalization Abilities: Sample Complexity Results. Nina Balcan (2015). Lecture notes.4/23 WedEXAM 2In-classLearning objectives:pdfPractice exam:pdf(sol)RecitationRecitation starts the first week of class, Friday, Jan. 17th. Recitation attendance is recommended to help solidify weekly course topics.
          That being said, the recitation materials published below are required content and are in scope for midterms 1 and 2.
          Students frequently say that recitations are one of the most important aspects of the course.Recitation section assignments will be locked down after the third week.
          Until then, you may try attending different recitation sections to find the best fit for you.
          In the case of any over-crowded recitation sections, priority goes to students that are officially registered for that section in SIO.
          The process to select your final recitation assignment will be announced onEdas we get closer to Recitation 4.Recitations will be on Fridays in the following individual recitation sections:SectionTimeLocationTAsAFriday 10:00 am - 10:50 amGHC 4102AJ and GaurikaBFriday 11:00 am - 11:50 amN/AMerged with Section CCFriday 11:00 am - 11:50 amGHC 4301Shreya and MargaretDFriday 12:00 pm - 12:50 pmGHC 4301Ethan and MargaretEFriday 1:00 pm - 1:50 pmPH 126AJerick and EthanFFriday 2:00 pm - 2:50 pmGHC 4102Derek and JerickDatesRecitationHandout/Code1/17 FriRecitation 1: NumPyhello_notebooks.ipynb1_Intro_to NumPy.ipynb(solution)2_Loading_and_Visualizing_Data.ipynb(solution)3_Messing_with_MNIST.ipynbAdditional reference:NumPy_Tutorial_from_11-785.ipynb1/24 FriRecitation 2: Decision TreesWorksheet:pdf(solution)kNN.ipynbDT.ipynb1/31 FriRecitation 3: Matrix Calculus and Linear Regressionpdf(solution)2/7 FriRecitation 4: Logistic Regressionpdf(solution)2/14 FriRecitation 5: Neural Networkspdf(solution)2/21 FriRecitation 6: Regularization, Prob/Stat ReviewWorksheet:pdf(solution)Notebook:Gaussian Contour Plots2/28 FriRecitation 7: MLENo in-person recitation (worksheet only)pdf(solution)3/7 FriNo recitation -- Spring Break3/14 FriRecitation 8: PyTorch, MAPWorksheet:pdf(solution)PyTorch Overview SlidesPyTorch Tutorial Notebook.ipynb3/21 FriRecitation 9: Computer VisionWorksheet:pdf(solution)3/28 FriRecitation 10: NLPWorksheet:pdf(solution)word_embeddings.ipynb(solution)minGPT_pico.ipynbminGPT_femto.ipynb4/4 FriRecitation 11: PCA, Recommender Systems, K-meansCarnival: No in-person recitation (worksheet only)Worksheet:pdf(solution)4/11 FriRecitation 12: Generative & Discriminative Models + MAPWorksheet:pdf(solution)discriminant_analysis.ipynb4/18 FriRecitation 13: Variational AutoencodersWorksheet:pdf(solution)kl_divergence.ipynb4/25 FriRecitation 14: Last Recitation! Mini-project Help SessionExamsThe course includes two midterm exams. The midterms will be on Feb. 26 and Apr. 23. Both will take place in class, 11:00 am-12:20 pm.
        Plan any travel around exams, as exams cannot be rescheduled. There is no final exam.Mini-projectA mini-project due during the final exam period. This will be an
          opportunity to work with a team and apply machine learning concepts
          from class to a project that is more customized to your interests.
          More details about the mini-project details and deadlines will be
          announce later in the semester.AssignmentsThere will be approximately five homework assignments that will have written and programming components and approximately six online assignments (subject to change). Written and online components will involve working through algorithms presented in the class, deriving and proving mathematical results, and critically analyzing material presented in class. Programming assignments will involve writing code in Python to implement various algorithms.For any assignments that aren't released yet, the dates below are tentative and subject to change.Assignment due dates (Tentative)AssignmentLink (if released)Due DateHW 0 (online)Gradescope1/23 Thu, 11:59 pmHW 1 (programming)hw1.ipynb1/23 Thu, 11:59 pmHW 2 (online)Gradescope1/30 Thu, 11:59 pmHW 3 (written/programming)hw3_blank.pdf,hw3_tex.zip,hw3.ipynb2/6 Thu, 11:59 pmHW 4 (online)Gradescope2/13 Thu, 11:59 pmHW 5 (written/programming)hw5_blank.pdf,hw5_tex.zip,hw5.ipynb2/21 Fri, 11:59 pmHW 6 (online)Gradescope3/20 Thu, 11:59 pmHW 7 (written/programming)hw7_blank.pdf,hw7_tex.zip,hw7.ipynb3/27 Thu, 11:59 pmHW 8 (online)Gradescope4/2 Wed, 11:59 pmHW 9 (online)Gradescope4/12 Sat, 11:59 pmHW 10 (written/programming)hw10_blank.pdf,hw10_tex.zip,hw10.ipynb4/18 Fri, 11:59 pmPre-reading due dates (Tentative)AssignmentLink (if released)Due DateDecision TreesCheckpoint1/21 Tue, 11:59 pmOpt and LinRegCheckpoint1/26 Sun, 11:59 pmFeature Eng., Logistic Reg.Checkpoint2/2 Sun, 11:59 pmNeural NetworksCheckpoint2/9 Sun, 11:59 pmRegularizationCANCELEDN/AMLECheckpoint2/23 Sun, 11:59 pmMAPCheckpoint3/9 Sun, 11:59 pmCNNsCheckpoint3/17 Mon, 10:30 amWord EmbeddingsReleased: TBDCANCELEDPCA, Recommender Sys., ClusteringCheckpoint3/30 Sun, 11:59 pmProbabilistic Generative ModelsCheckpoint4/6 Sun, 11:59 pmVariational AutoencodersCheckpoint4/14 Mon, 10:30 amProject due datesAssignmentLink (if released)Due DatePrjoject Group SelectedGoogle Form4/2 Thur, 11:59 pmPrjoject Proposal with DataGoogle Form4/9 Wed, 11:59 pmFinal Project SubmissionGoogle Form5/2 Fri, 11:59 pmPoliciesGradingGrades will ultimately be collected and reported inCanvas.Final scores will be composed of:20% Midterm Exam 120% Midterm Exam 235% Written/Programming homework5% Online homework5% Participation5% Pre-Lecture Reading Checkpoints10% Mini-projectFinal GradeThis class is not curved. However, we convert final course scores to letter grades based on grade boundaries that are determined at the end of the semester. What follows is arough guideto how course grades will be established, not a precise formula — we will fine-tune cutoffs and other details as we see fit after the end of the course. This is meant to help you set expectations and take action if your trajectory in the class does not take you to the grade you are hoping for. So, here's aroughheuristics about the correlation between final grades and total scores:A: above 90%B: 80-90%C: 70-80%D: 60-70%This heuristic assumes that the makeup of a student's grade is not wildly anomalous: exceptionally low overall scores on exams, programming assignments, or written assignments will be treated on a case-by-case basis and, while rare, could potentially drop a students grade.Precise grade cutoffs will not be discussed at any point during or after the semester. For students very close to grade boundaries, instructors may, at their discretion, consider participation in lecture and recitation, exam performance, and overall grade trends when assigning the final grade.ParticipationIn class, we will use a series of polls as part of an active learning technique calledPeer Instruction. Your participation grade will be based on the percentage of these in-class poll questions answered:0% for 50% or less5% for 80% or greater poll participationLinear scale for values in between 50% and 80%Correctness of in-class polling responses will not be taken into account for participation grades.If a poll is duplicated with the same question (e.g. before and after discussing with your neighbor), you should answer all of the duplicated versions as well, as they will be counted as separate polls.If you have systemic/repeated technical issues, please let us know as soon as possible, so we can resolve the situation.Missing polls due to absences (e.g., brief illness) from lecture or due to technical difficulties is expected occasionally, and this is why you only need to answer >= 80% of the polls to get full credit.It is against the course academic integrity policy to answer in-class
        polls when you are not present in lecture. Violations of this policy
        will be reported as an academic integrity violation. Information about
        academic integrity at CMU may be found athttps://www.cmu.edu/academic-integrity.Late Policies, and Extensions, and ExceptionsParticipationMissing polls due to absences (e.g., brief illness) from lecture or due to technical difficulties is expected occasionally, and this is why you only need to answer >= 80% of the polls to get full credit.If you must miss many lectures due to circumstances outside of your control (e.g., if you have an extended illness) please e-mail Nichelle, nichellp@andrew.cmu.edu, prior to lecture.Pre-reading checkpointsPre-reading checkpoints don't have any extensions or late days.
          However, the lowest two checkpoints will be dropped when computing
          your semester score.Reasoning: We want to make sure that
          everyone is able to complete the pre-reading prior to lecture, so we
          can build on that knowledge in class; minor illness and other minor
          disruptive events outside of your control happen occasionally and
          thus dropping the lowest two scores. See below for information on
          rare exceptions.Written/programming homework and online homeworkYou have a pool of 6 late days across all written/programming and
          online assignment typesUse up to two per assignmentWritten and programming assignments with the same homework number are considered the same assignment; so e.g., if you turn in both programming and written components within 24 hours after the due date, you will use one late day, not two.You may use these at your discretion, but they are intended for minor illness and other disruptive events outside of your control, and not for poor time management.No need to inform us that you are using a late day; just submit it to Gradescope during the late day.You are responsible to keep track of your own late days. Gradescope will not enforce the total number of late daysHomework submitted after these two late days or submitted by a student without any late days remaining will be given a score of 0.Exceptions and extensionsAside from late days, dropping the lowest checkpoints, and the 80%
          threshold for participation, there will be no extensions on
          assignments in general. If you think you reallyreallyneed
          an extension on a particular assignment, e-mail Nichelle, nichellp@andrew.cmu.edu,
          as soon as possible and before the deadline. Please be aware that
          extensions are entirely discretionary and will be granted only in
          exceptional circumstances outside of your control (e.g., due to severe
          illness or major personal/family emergencies, but not for competitions,
          club-related events, or interviews). The instructors will require confirmation
          from your academic advisor, as appropriate.We certainly understand that unfortunate things happen in life. However,
        not all unfortunate circumstances are valid reasons for an extension.
        Nearly all situations that make you run late on an assignment homework
        can be avoided with proper planning - often just starting early. Here
        are some examples:I have so many deadlines this week: you know your deadlines ahead of time - plan accordingly.It's a minute before the deadline and the network is down: you always have multiple submissions - it's not a good idea to wait for the deadline for your first submission.My computer crashed and I lost everything: Use Google Drive, Dropbox, or similar system to do real-time backup - recover your files and finish your homework from a cluster machine or borrowed computer.Collaboration and Academic Integrity PoliciesCollaborationThe purpose of student collaboration is to facilitate learning, not
        to circumvent it. Studying the material in groups is strongly
        encouraged. You are also allowed to seek help from other students in
        understanding the material needed to solve a particular homework
        problem, provided any written notes (including code) are taken on an
        impermanent surface (e.g. whiteboard, chalkboard), and provided
        learning is facilitated, not circumvented. The actual solution must
        be done by each student alone.A good method to follow when collaborating is to meet with your
        peers, discuss ideas at a high level, but do not copy down any notes
        from each other or from a white board. Any scratch work done at this
        time should be your own only. Before writing the assignment
        solutions, you should make sure that you are doing this without
        anyone else present, putting all notes away, closing all tabs on
        your computer, and writing it completely by yourself with no other
        resources.Youmay NOTview, share, or communicate about any artifact that will be submitted as part of an assignment. Example artifacts include, but are not limited to: code, pseudocode, diagrams, and text.Youmaylook at another student'scode outputand discuss it at a conceptual level, as long as it is not output that appears directly in the homework submission.Youmaylook at another student's code error messages and discuss what the error means at a conceptual level. However, youmay NOTgive specific instructions to fix the error.All work that you present must be your own. Auto-generated code, for example, is not acceptable.Using any external sources of code or algorithms in any way must have approval from the instructorbeforesubmitting the work. For example, you must get instructor approval before using an algorithm you found online for implementing a optmization function in a programming assignment.The presence or absence of any form of help or collaboration,
        whether given or received, must be explicitly stated and disclosed
        in full by all involved. Specifically, each assignment solution must
        include answering the following questions:Did you receive any help whatsoever from anyone in solving this assignment?   Yes / No.If you answered 'yes', give full details:____________(e.g. “Jane Doe explained to me what is asked in Question 3.4”)Did you give any help whatsoever to anyone in solving this assignment?           Yes / No.If you answered 'yes', give full details:_____________(e.g. “I pointedFull Nameto section 2.3 since they didn't know how to proceed with Question 2”)Did you find or come across code that implements any part of this assignment ?  Yes / No.    (See below policy on “found code”)If you answered 'yes', give full details:_____________(book & page, URL & location within the page, etc.).If you gave help after turning in your own assignment and/or after
        answering the questions above, you must update your answers before
        the assignment's deadline, if necessary by emailing Nichelle.Collaboration without full disclosure will be handled severely, in
        compliance withCMU's Policy on Academic
        Integrity.AI AssistanceTo best support your own learning, you should complete all graded
        assignments in this course yourself, without any use of generative
        artificial intelligence (AI), such as ChatGPT. Please refrain from
        using AI tools to generate any content (text, video, audio, images,
        code, etc.) for an assessment. Passing off any AI generated content as
        your own (e.g., cutting and pasting content into written assignments,
        or paraphrasing AI content) constitutes a violation of CMU's academic
        integrity policy.Policy Regarding “Found Code”You are encouraged to read books and other instructional materials,
        both online and offline, to help you understand the concepts and
        algorithms taught in class. These materials may contain example code
        or pseudo code, which may help you better understand an algorithm or
        an implementation detail. However, when you implement your own
        solution to an assignment, you must put all materials aside, and write
        your code completely on your own, starting “from
        scratch”. Specifically, you may not use any code you found or came
        across. If you find or come across code that implements any part of
        your assignment, you must disclose this fact in your collaboration
        statement.Duty to Protect One's WorkStudents are responsible for pro-actively protecting their work from
        copying and misuse by other students. If a student's work is copied by
        another student, the original author is also considered to be at fault
        and in gross violation of the course policies. It does not matter
        whether the author allowed the work to be copied or was merely
        negligent in preventing it from being copied. When overlapping work is
        submitted by different students, both students will be punished.Do not post your solutions publicly,
        neither during the course nor afterwards.Penalties for Violations of Course PoliciesViolations of these policies will be reported as an academic integrity violation and will also result in a -100% score on the associated assignment/exam. Information about academic integrity at CMU may be found athttps://www.cmu.edu/academic-integrity. Please contact the instructor if you ever have any questions regarding academic integrity or these collaboration policies.(The above policies are adapted from10-601 Fall 2018and10-301/601 Fall 2023course policies.)Accommodations for Students with DisabilitiesIf you have a disability and have an accommodations letter from the Disability Resources office, we encourage you to discuss your accommodations and needs with us as early in the semester as possible. We will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, we encourage you to visit theirwebsite.Statement of Support for Students' Health & Well-beingTake care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, getting enough sleep, and taking some time to relax. This will help you achieve your goals and cope with stress.All of us benefit from support during times of struggle. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is almost always helpful.If you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 and visit their website athttp://www.cmu.edu/counseling/. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help.If you have questions about this or your coursework, please let us know. Thank you, and have a great semester.

LEARNING OBJECTIVES:
--------------------------------------------------------------------------------
After completing the course, students should be able to:Select and apply an appropriate supervised learning algorithm for classification and regression problems (e.g., linear regression, logistic regression, ridge regression, nonparametric kernel regression, neural networks, naive Bayes).Recognize different types of unsupervised learning problems, and select and apply appropriate algorithms (e.g., k-means clustering, Gaussian mixture models, linear and nonlinear dimensionality reduction).Work with probability (Bayes rule, conditioning, expectations, independence), linear algebra (vector and matrix operations, eigenvectors, SVD), and calculus (gradients, Jacobians) to derive machine learning methods such as linear regression, naive Bayes, and principal component analysis.Understand machine learning principles such as model selection, overfitting, and underfitting, and techniques such as cross-validation,regularization, feature learning, fine-tuning, and transfer learning.Implement machine learning algorithms such as logistic regression via stochastic gradient descent, linear regression, or k-means clustering.Work with machine learning toolkits, such as PyTorch, to implement, train, and analyze various deep learning networks, including convolutional neural networks and transformer architectures.Run appropriate supervised and unsupervised learning algorithms on real and synthetic data sets and interpret the results.

PREREQUISITES:
--------------------------------------------------------------------------------
The prequisites for this course are:15-122:Principles of Imperative Computation15-151 or 21-127 or 21-128:Mathematical Foundations of Computer Science / Concepts of Mathematics.36-225 or 36-218 or 36-217 or 15-259 or 15-359 or 21-325 or 36-219: Probability21-241 or 21-240 or 21-242: Linear Algebra[Implied from prereqs above] Calc II Integration and Approximation
While not explicitly a prerequisite, we will be programming exclusively in Python. Please see the instructor if you are unsure whether your background is suitable for the course.

TOPICS COVERED:
--------------------------------------------------------------------------------
10-315, Spring 2025Introduction to Machine Learning (SCS majors)OverviewKey InformationLectureMonday + Wednesday, 11:00 am - 12:20 pm, Wean Hall 7500RecitationSection A, Friday 10:00 am - 10:50 am, GHC 4102, seeRecitationSection B/C, Friday 11:00 am - 11:50 am, GHC 4301Section D, Friday 12:00 pm - 12:50 pm, GHC 4301Section E, Friday 1:00 pm - 1:50 pm, PH 126ASection F, Friday 2:00 pm - 2:50 pm, GHC 4102InstructorPat VirtueEducation AssociateNichelle PhillipsTeaching AssistantsMargaret He,
                    Derek Yuan,
                    Shreya Sridhar,
                    Gaurika Sawhney,
                    Jerick Shi,
                    Ethan Wang,
                    AJ Seo, see the315 Staff pageGradingGrades will be collected inCanvas.Midterm 1 20%, Midterm 2 20%, Written/Programming Homework 35%, Pre-Lecture Reading Checkpoints 5%, Online Homework 5%, Participation 5%, Mini-project 10%TextbookThere is no required textbook for this course. Any recommended readings will come from sources freely available online.Announcements + Q&AWe will useEdfor questions and any course announcements.Submitting AssignmentsStudents will turn in their homework electronically usingGradescope.Machine Learning is concerned with computer programs that automatically improve their performance through experience (e.g., programs that learn to recognize human faces, recommend music and movies, and drive autonomous robots).
          This course covers the core concepts, theory, algorithms and applications of machine learning.Learning ObjectivesAfter completing the course, students should be able to:Select and apply an appropriate supervised learning algorithm for classification and regression problems (e.g., linear regression, logistic regression, ridge regression, nonparametric kernel regression, neural networks, naive Bayes).Recognize different types of unsupervised learning problems, and select and apply appropriate algorithms (e.g., k-means clustering, Gaussian mixture models, linear and nonlinear dimensionality reduction).Work with probability (Bayes rule, conditioning, expectations, independence), linear algebra (vector and matrix operations, eigenvectors, SVD), and calculus (gradients, Jacobians) to derive machine learning methods such as linear regression, naive Bayes, and principal component analysis.Understand machine learning principles such as model selection, overfitting, and underfitting, and techniques such as cross-validation,regularization, feature learning, fine-tuning, and transfer learning.Implement machine learning algorithms such as logistic regression via stochastic gradient descent, linear regression, or k-means clustering.Work with machine learning toolkits, such as PyTorch, to implement, train, and analyze various deep learning networks, including convolutional neural networks and transformer architectures.Run appropriate supervised and unsupervised learning algorithms on real and synthetic data sets and interpret the results.LevelsThis course is designed for SCS undergraduate majors. It covers many similar topics to other introductory machine learning course, such as 10-301/10-601 and 10-701. This 10-315 course and 15-281 AI Representation and Problem Solving are designed to complement each other and provide both breadth and depth across AI and ML topics. Contact the instructor if you are concerned about which machine learning course is appropriate for you.PrerequisitesThe prequisites for this course are:15-122:Principles of Imperative Computation15-151 or 21-127 or 21-128:Mathematical Foundations of Computer Science / Concepts of Mathematics.36-225 or 36-218 or 36-217 or 15-259 or 15-359 or 21-325 or 36-219: Probability21-241 or 21-240 or 21-242: Linear Algebra[Implied from prereqs above] Calc II Integration and ApproximationWhile not explicitly a prerequisite, we will be programming exclusively in Python. Please see the instructor if you are unsure whether your background is suitable for the course.Office HoursOHQueue:LinkPat's Office HoursIn addition to Pat's standing office hourse, he often have "OH" (or "Open") appointment slots on hisoffice hours appointment calendar.
        If no there are no available OH or appointments that meet your needs, please contact Pat via a private post onEdwith a list of times that work for you to meet.ScheduleSubject to changeTextbooks:Bishop, Christopher.Pattern Recognition and Machine Learning,available onlineDaumé III, Hal.A Course in Machine Learning,available online(DL) Goodfellow, Ian, Yoshua Bengio, Aaron Courville.Deep Learning,available online(MML) Marc Peter Deisenroth, A. Aldo Faisal, and Cheng Soon Ong.Mathematics for Machine Learning,available onlineMitchell, Tom.Machine Learning,available onlineMurphy, Kevin P.Machine Learning: A Probabilistic Perspective,available online(KMPA) Shaw-Taylor, John, Nello Cristianini.Kernel Methods for Pattern Analysis,available onlineDatesTopicLecture MaterialsPre-ReadingReading (optional)1/13 Mon1. IntroductionNotation_Guide.pdfMath_Background.pdfpptx(inked)pdf(inked)MML2.1-3, 2.5, 2.6 and 3.1, 3.2.1, 3.31/15 Wed2. ML Problem Formulationpptx(inked)pdf(inked)Mitchell1.1-1.2Daumé 11/20 MonNo class: MLK Day1/22 Wed3. Decision Treespptx(inked)pdf(inked)Decision Trees.pdfCheckpointdue 1/21 Tue, 11:59 pmDaumé 2,Entropy, Cross-Entropy video, A. GéronPaper:ID31/27 Mon4. K-Nearest Neighbor andModel Selectionpptx(inked)pdf(inked)pptx(inked)pdf(inked)Opt and LinReg.pdfCheckpointdue 1/26 Sun, 11:59 pmDaumé 3MML8.3.31/29 Wed5. Optimization and Linear Regressionpptx(inked)pdf(inked)regression interactive.ipynbregression blind interactive.ipynbMML8.2-8.2.2, 8.2.4MML5.2-5.52/3 Mon6. Optimization and Linear Regression (cont.)See previous lecture slidesFeatEng and LogReg.pdfCheckpointdue 2/2 Sun, 11:59 pm2/5 Wed7. Feature EngineeringLogistic Regressionpptx(inked)pdf(inked)Demos:Linear/Logistic:notebook,Desmos(3D)Quadratic/Logistic:notebook,Desmos 3DMulticlass logistic DesmosBishop4.1.3, 4.3.2, 4.3.42/10 Mon8. Logistic Regression (cont.)pptx(inked)pdf(inked)Convex functions DesmosNeural Networks.pdfCheckpointdue 2/9 Sun, 11:59 pm2/12 Wed9. Neural Networkspptx(inked)pdf(inked)three neuron interactive.ipynbMML5.6DL 6The Matrix Cookbook2/17 Mon10. Neural Networks (cont.)pptx(inked)pdf(inked)Universal network DesmosPerceptron neuron Desmos2/19 Wed11. Regularizationpptx(inked)pdf(inked)regression regularization.ipynbRegularization interpolation Desmos(3D)L1_sparsity.ipynbDL 7.1,7.8Bishop3.1.42/24 Mon12. MLE and Probabilistic Modelingpptx(inked)pdf(inked)MLE.pdfCheckpointdue 2/23 Sun, 11:59 pmMML9-9.2.2Bishop1.2.4-5, 3.1.1-22/26 WedEXAM 1In-classLearning objectives:pdfPractice exam:pdf(sol)3/3 MonNo class: Spring Break3/5 WedNo class: Spring Break3/10 Mon13. MLE (cont.) & MAPpptx(inked)pdf(inked)Desmos: Bernoulli Likelihood and PosteriorMAP.pdfCheckpointdue 3/9 Sun, 11:59 pmMML9.2.3-4Mitchell MLE and MAP3/12 Wed14. MAP (cont.)3/17 Mon15. Practical & Responsible MLPyTorch, ML Model Cardspptx(inked)pdf(inked)Demos:autoencoder csv.ipynbdigit logistic regression.ipynbdigit net.ipynbCNNs.pdfCheckpointdue3/17 Mon, 10:30 amModel Cards For Model Reporting. Margaret Mitchell, et al (2019)PyTorch Basics Tutorial3/19 Wed16. Deep Learning for Computer Visionpptx(inked)pdf(inked)DL 93/24 Mon17. Natrual Language Processing: Features, N-gramspptx(inked)pdf(inked)Demo:N-gramsWord Embeddings_v0_1.pdfNo checkpoint3/26 Wed18. NLP: Word Embeddings, Attentionpptx(inked)pdf(inked)Demos:Cosine SimilarityRotary Position Encoding 2DThe IllustratedWord2vec. Jay AlammarThe Illustrated {Attention→Transformer→GPT-2}. Jay AlammarVideo (and code):Let's build GPT. Andrej Karpathy3/31 Mon19. NLP (cont.): Transformers, LLMspptx(inked)pdf(inked)Demos:Linear TransformsLinear Transforms (words)PCA, Recommender Systems, ClusteringCheckpointdue 3/30 Sun, 11:59 pm4/2 Wed20. Dimensionality Reduction: PCA, Autoencoders, Feature Learningpptx(inked)pdf(inked)
              Demos:Desmos: Projectionpca_2d_exercise.ipynbautoencoder_csv.ipynbPCA:Bishop12.1,Murphy12.2Murphy25.54/7 Mon21. PCA (cont.) & Clusteringpptx(inked)pdf(inked)Generative.pdfCheckpointdue 4/6 Sun, 11:59 pmK-means:Bishop9.1,4/9 Wed22. Recommender Systems &Probabilistic Generative ModelsRecommender Systemspptx(inked)pdf(inked)Probabilistic Generative Modelspptx(inked)pdf(inked)Demos:cat_dog_sampling.ipynbdiscriminant_analysis.ipynbMatrix Factorization Techniques for Recommender Systems(pdf). Koren, Bell, and Volinsky (2009)Generative models:Mitchell Generative and Discriminative ClassifiersMurphy3.5, 4.2, 8.64/14 Mon23. Variational Autoencoderspptx(inked)pdf(inked)VAE.pdfCheckpointdue4/14 Mon, 10:30 amAuto-Encoding Variational BayesKingma and Welling, 2013.Tutorial on Variational AutoencodersDoersch, 2016.4/16 Wed24. Diffusion4/21 Mon25. Learning TheoryA Few Useful Things to Know about Machine Learning. Pedro Domingos (2012).Generalization Abilities: Sample Complexity Results. Nina Balcan (2015). Lecture notes.4/23 WedEXAM 2In-classLearning objectives:pdfPractice exam:pdf(sol)RecitationRecitation starts the first week of class, Friday, Jan. 17th. Recitation attendance is recommended to help solidify weekly course topics.
          That being said, the recitation materials published below are required content and are in scope for midterms 1 and 2.
          Students frequently say that recitations are one of the most important aspects of the course.Recitation section assignments will be locked down after the third week.
          Until then, you may try attending different recitation sections to find the best fit for you.
          In the case of any over-crowded recitation sections, priority goes to students that are officially registered for that section in SIO.
          The process to select your final recitation assignment will be announced onEdas we get closer to Recitation 4.Recitations will be on Fridays in the following individual recitation sections:SectionTimeLocationTAsAFriday 10:00 am - 10:50 amGHC 4102AJ and GaurikaBFriday 11:00 am - 11:50 amN/AMerged with Section CCFriday 11:00 am - 11:50 amGHC 4301Shreya and MargaretDFriday 12:00 pm - 12:50 pmGHC 4301Ethan and MargaretEFriday 1:00 pm - 1:50 pmPH 126AJerick and EthanFFriday 2:00 pm - 2:50 pmGHC 4102Derek and JerickDatesRecitationHandout/Code1/17 FriRecitation 1: NumPyhello_notebooks.ipynb1_Intro_to NumPy.ipynb(solution)2_Loading_and_Visualizing_Data.ipynb(solution)3_Messing_with_MNIST.ipynbAdditional reference:NumPy_Tutorial_from_11-785.ipynb1/24 FriRecitation 2: Decision TreesWorksheet:pdf(solution)kNN.ipynbDT.ipynb1/31 FriRecitation 3: Matrix Calculus and Linear Regressionpdf(solution)2/7 FriRecitation 4: Logistic Regressionpdf(solution)2/14 FriRecitation 5: Neural Networkspdf(solution)2/21 FriRecitation 6: Regularization, Prob/Stat ReviewWorksheet:pdf(solution)Notebook:Gaussian Contour Plots2/28 FriRecitation 7: MLENo in-person recitation (worksheet only)pdf(solution)3/7 FriNo recitation -- Spring Break3/14 FriRecitation 8: PyTorch, MAPWorksheet:pdf(solution)PyTorch Overview SlidesPyTorch Tutorial Notebook.ipynb3/21 FriRecitation 9: Computer VisionWorksheet:pdf(solution)3/28 FriRecitation 10: NLPWorksheet:pdf(solution)word_embeddings.ipynb(solution)minGPT_pico.ipynbminGPT_femto.ipynb4/4 FriRecitation 11: PCA, Recommender Systems, K-meansCarnival: No in-person recitation (worksheet only)Worksheet:pdf(solution)4/11 FriRecitation 12: Generative & Discriminative Models + MAPWorksheet:pdf(solution)discriminant_analysis.ipynb4/18 FriRecitation 13: Variational AutoencodersWorksheet:pdf(solution)kl_divergence.ipynb4/25 FriRecitation 14: Last Recitation! Mini-project Help SessionExamsThe course includes two midterm exams. The midterms will be on Feb. 26 and Apr. 23. Both will take place in class, 11:00 am-12:20 pm.
        Plan any travel around exams, as exams cannot be rescheduled. There is no final exam.Mini-projectA mini-project due during the final exam period. This will be an
          opportunity to work with a team and apply machine learning concepts
          from class to a project that is more customized to your interests.
          More details about the mini-project details and deadlines will be
          announce later in the semester.AssignmentsThere will be approximately five homework assignments that will have written and programming components and approximately six online assignments (subject to change). Written and online components will involve working through algorithms presented in the class, deriving and proving mathematical results, and critically analyzing material presented in class. Programming assignments will involve writing code in Python to implement various algorithms.For any assignments that aren't released yet, the dates below are tentative and subject to change.Assignment due dates (Tentative)AssignmentLink (if released)Due DateHW 0 (online)Gradescope1/23 Thu, 11:59 pmHW 1 (programming)hw1.ipynb1/23 Thu, 11:59 pmHW 2 (online)Gradescope1/30 Thu, 11:59 pmHW 3 (written/programming)hw3_blank.pdf,hw3_tex.zip,hw3.ipynb2/6 Thu, 11:59 pmHW 4 (online)Gradescope2/13 Thu, 11:59 pmHW 5 (written/programming)hw5_blank.pdf,hw5_tex.zip,hw5.ipynb2/21 Fri, 11:59 pmHW 6 (online)Gradescope3/20 Thu, 11:59 pmHW 7 (written/programming)hw7_blank.pdf,hw7_tex.zip,hw7.ipynb3/27 Thu, 11:59 pmHW 8 (online)Gradescope4/2 Wed, 11:59 pmHW 9 (online)Gradescope4/12 Sat, 11:59 pmHW 10 (written/programming)hw10_blank.pdf,hw10_tex.zip,hw10.ipynb4/18 Fri, 11:59 pmPre-reading due dates (Tentative)AssignmentLink (if released)Due DateDecision TreesCheckpoint1/21 Tue, 11:59 pmOpt and LinRegCheckpoint1/26 Sun, 11:59 pmFeature Eng., Logistic Reg.Checkpoint2/2 Sun, 11:59 pmNeural NetworksCheckpoint2/9 Sun, 11:59 pmRegularizationCANCELEDN/AMLECheckpoint2/23 Sun, 11:59 pmMAPCheckpoint3/9 Sun, 11:59 pmCNNsCheckpoint3/17 Mon, 10:30 amWord EmbeddingsReleased: TBDCANCELEDPCA, Recommender Sys., ClusteringCheckpoint3/30 Sun, 11:59 pmProbabilistic Generative ModelsCheckpoint4/6 Sun, 11:59 pmVariational AutoencodersCheckpoint4/14 Mon, 10:30 amProject due datesAssignmentLink (if released)Due DatePrjoject Group SelectedGoogle Form4/2 Thur, 11:59 pmPrjoject Proposal with DataGoogle Form4/9 Wed, 11:59 pmFinal Project SubmissionGoogle Form5/2 Fri, 11:59 pmPoliciesGradingGrades will ultimately be collected and reported inCanvas.Final scores will be composed of:20% Midterm Exam 120% Midterm Exam 235% Written/Programming homework5% Online homework5% Participation5% Pre-Lecture Reading Checkpoints10% Mini-projectFinal GradeThis class is not curved. However, we convert final course scores to letter grades based on grade boundaries that are determined at the end of the semester. What follows is arough guideto how course grades will be established, not a precise formula — we will fine-tune cutoffs and other details as we see fit after the end of the course. This is meant to help you set expectations and take action if your trajectory in the class does not take you to the grade you are hoping for. So, here's aroughheuristics about the correlation between final grades and total scores:A: above 90%B: 80-90%C: 70-80%D: 60-70%This heuristic assumes that the makeup of a student's grade is not wildly anomalous: exceptionally low overall scores on exams, programming assignments, or written assignments will be treated on a case-by-case basis and, while rare, could potentially drop a students grade.Precise grade cutoffs will not be discussed at any point during or after the semester. For students very close to grade boundaries, instructors may, at their discretion, consider participation in lecture and recitation, exam performance, and overall grade trends when assigning the final grade.ParticipationIn class, we will use a series of polls as part of an active learning technique calledPeer Instruction. Your participation grade will be based on the percentage of these in-class poll questions answered:0% for 50% or less5% for 80% or greater poll participationLinear scale for values in between 50% and 80%Correctness of in-class polling responses will not be taken into account for participation grades.If a poll is duplicated with the same question (e.g. before and after discussing with your neighbor), you should answer all of the duplicated versions as well, as they will be counted as separate polls.If you have systemic/repeated technical issues, please let us know as soon as possible, so we can resolve the situation.Missing polls due to absences (e.g., brief illness) from lecture or due to technical difficulties is expected occasionally, and this is why you only need to answer >= 80% of the polls to get full credit.It is against the course academic integrity policy to answer in-class
        polls when you are not present in lecture. Violations of this policy
        will be reported as an academic integrity violation. Information about
        academic integrity at CMU may be found athttps://www.cmu.edu/academic-integrity.Late Policies, and Extensions, and ExceptionsParticipationMissing polls due to absences (e.g., brief illness) from lecture or due to technical difficulties is expected occasionally, and this is why you only need to answer >= 80% of the polls to get full credit.If you must miss many lectures due to circumstances outside of your control (e.g., if you have an extended illness) please e-mail Nichelle, nichellp@andrew.cmu.edu, prior to lecture.Pre-reading checkpointsPre-reading checkpoints don't have any extensions or late days.
          However, the lowest two checkpoints will be dropped when computing
          your semester score.Reasoning: We want to make sure that
          everyone is able to complete the pre-reading prior to lecture, so we
          can build on that knowledge in class; minor illness and other minor
          disruptive events outside of your control happen occasionally and
          thus dropping the lowest two scores. See below for information on
          rare exceptions.Written/programming homework and online homeworkYou have a pool of 6 late days across all written/programming and
          online assignment typesUse up to two per assignmentWritten and programming assignments with the same homework number are considered the same assignment; so e.g., if you turn in both programming and written components within 24 hours after the due date, you will use one late day, not two.You may use these at your discretion, but they are intended for minor illness and other disruptive events outside of your control, and not for poor time management.No need to inform us that you are using a late day; just submit it to Gradescope during the late day.You are responsible to keep track of your own late days. Gradescope will not enforce the total number of late daysHomework submitted after these two late days or submitted by a student without any late days remaining will be given a score of 0.Exceptions and extensionsAside from late days, dropping the lowest checkpoints, and the 80%
          threshold for participation, there will be no extensions on
          assignments in general. If you think you reallyreallyneed
          an extension on a particular assignment, e-mail Nichelle, nichellp@andrew.cmu.edu,
          as soon as possible and before the deadline. Please be aware that
          extensions are entirely discretionary and will be granted only in
          exceptional circumstances outside of your control (e.g., due to severe
          illness or major personal/family emergencies, but not for competitions,
          club-related events, or interviews). The instructors will require confirmation
          from your academic advisor, as appropriate.We certainly understand that unfortunate things happen in life. However,
        not all unfortunate circumstances are valid reasons for an extension.
        Nearly all situations that make you run late on an assignment homework
        can be avoided with proper planning - often just starting early. Here
        are some examples:I have so many deadlines this week: you know your deadlines ahead of time - plan accordingly.It's a minute before the deadline and the network is down: you always have multiple submissions - it's not a good idea to wait for the deadline for your first submission.My computer crashed and I lost everything: Use Google Drive, Dropbox, or similar system to do real-time backup - recover your files and finish your homework from a cluster machine or borrowed computer.Collaboration and Academic Integrity PoliciesCollaborationThe purpose of student collaboration is to facilitate learning, not
        to circumvent it. Studying the material in groups is strongly
        encouraged. You are also allowed to seek help from other students in
        understanding the material needed to solve a particular homework
        problem, provided any written notes (including code) are taken on an
        impermanent surface (e.g. whiteboard, chalkboard), and provided
        learning is facilitated, not circumvented. The actual solution must
        be done by each student alone.A good method to follow when collaborating is to meet with your
        peers, discuss ideas at a high level, but do not copy down any notes
        from each other or from a white board. Any scratch work done at this
        time should be your own only. Before writing the assignment
        solutions, you should make sure that you are doing this without
        anyone else present, putting all notes away, closing all tabs on
        your computer, and writing it completely by yourself with no other
        resources.Youmay NOTview, share, or communicate about any artifact that will be submitted as part of an assignment. Example artifacts include, but are not limited to: code, pseudocode, diagrams, and text.Youmaylook at another student'scode outputand discuss it at a conceptual level, as long as it is not output that appears directly in the homework submission.Youmaylook at another student's code error messages and discuss what the error means at a conceptual level. However, youmay NOTgive specific instructions to fix the error.All work that you present must be your own. Auto-generated code, for example, is not acceptable.Using any external sources of code or algorithms in any way must have approval from the instructorbeforesubmitting the work. For example, you must get instructor approval before using an algorithm you found online for implementing a optmization function in a programming assignment.The presence or absence of any form of help or collaboration,
        whether given or received, must be explicitly stated and disclosed
        in full by all involved. Specifically, each assignment solution must
        include answering the following questions:Did you receive any help whatsoever from anyone in solving this assignment?   Yes / No.If you answered 'yes', give full details:____________(e.g. “Jane Doe explained to me what is asked in Question 3.4”)Did you give any help whatsoever to anyone in solving this assignment?           Yes / No.If you answered 'yes', give full details:_____________(e.g. “I pointedFull Nameto section 2.3 since they didn't know how to proceed with Question 2”)Did you find or come across code that implements any part of this assignment ?  Yes / No.    (See below policy on “found code”)If you answered 'yes', give full details:_____________(book & page, URL & location within the page, etc.).If you gave help after turning in your own assignment and/or after
        answering the questions above, you must update your answers before
        the assignment's deadline, if necessary by emailing Nichelle.Collaboration without full disclosure will be handled severely, in
        compliance withCMU's Policy on Academic
        Integrity.AI AssistanceTo best support your own learning, you should complete all graded
        assignments in this course yourself, without any use of generative
        artificial intelligence (AI), such as ChatGPT. Please refrain from
        using AI tools to generate any content (text, video, audio, images,
        code, etc.) for an assessment. Passing off any AI generated content as
        your own (e.g., cutting and pasting content into written assignments,
        or paraphrasing AI content) constitutes a violation of CMU's academic
        integrity policy.Policy Regarding “Found Code”You are encouraged to read books and other instructional materials,
        both online and offline, to help you understand the concepts and
        algorithms taught in class. These materials may contain example code
        or pseudo code, which may help you better understand an algorithm or
        an implementation detail. However, when you implement your own
        solution to an assignment, you must put all materials aside, and write
        your code completely on your own, starting “from
        scratch”. Specifically, you may not use any code you found or came
        across. If you find or come across code that implements any part of
        your assignment, you must disclose this fact in your collaboration
        statement.Duty to Protect One's WorkStudents are responsible for pro-actively protecting their work from
        copying and misuse by other students. If a student's work is copied by
        another student, the original author is also considered to be at fault
        and in gross violation of the course policies. It does not matter
        whether the author allowed the work to be copied or was merely
        negligent in preventing it from being copied. When overlapping work is
        submitted by different students, both students will be punished.Do not post your solutions publicly,
        neither during the course nor afterwards.Penalties for Violations of Course PoliciesViolations of these policies will be reported as an academic integrity violation and will also result in a -100% score on the associated assignment/exam. Information about academic integrity at CMU may be found athttps://www.cmu.edu/academic-integrity. Please contact the instructor if you ever have any questions regarding academic integrity or these collaboration policies.(The above policies are adapted from10-601 Fall 2018and10-301/601 Fall 2023course policies.)Accommodations for Students with DisabilitiesIf you have a disability and have an accommodations letter from the Disability Resources office, we encourage you to discuss your accommodations and needs with us as early in the semester as possible. We will work with you to ensure that accommodations are provided as appropriate. If you suspect that you may have a disability and would benefit from accommodations but are not yet registered with the Office of Disability Resources, we encourage you to visit theirwebsite.Statement of Support for Students' Health & Well-beingTake care of yourself. Do your best to maintain a healthy lifestyle this semester by eating well, exercising, getting enough sleep, and taking some time to relax. This will help you achieve your goals and cope with stress.All of us benefit from support during times of struggle. There are many helpful resources available on campus and an important part of the college experience is learning how to ask for help. Asking for support sooner rather than later is almost always helpful.If you or anyone you know experiences any academic stress, difficult life events, or feelings like anxiety or depression, we strongly encourage you to seek support. Counseling and Psychological Services (CaPS) is here to help: call 412-268-2922 and visit their website athttp://www.cmu.edu/counseling/. Consider reaching out to a friend, faculty or family member you trust for help getting connected to the support that can help.If you have questions about this or your coursework, please let us know. Thank you, and have a great semester.

